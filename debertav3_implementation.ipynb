{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a56d2-0cab-456a-a292-325f044f76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from transformers import DebertaV2Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e92aeba-5b59-4e2a-bd0f-c6e31350807a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CASEID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "list",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "4c092855-0209-441f-8be0-047c00db0052",
       "rows": [
        [
         "0",
         "2061638667",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "1",
         "2056600635",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "2",
         "2058253621",
         "Free nation where citizens elect their representatives.",
         "[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
        ],
        [
         "3",
         "2058997303",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "4",
         "2058184341",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "5",
         "2057930711",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "6",
         "2058524165",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "7",
         "2057837907",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "8",
         "2058736151",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ],
        [
         "9",
         "2057900787",
         "__NA__",
         "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASEID</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2061638667</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2056600635</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2058253621</td>\n",
       "      <td>Free nation where citizens elect their represe...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2058997303</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2058184341</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2057930711</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2058524165</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2057837907</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2058736151</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2057900787</td>\n",
       "      <td>__NA__</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CASEID                                       comment_text  \\\n",
       "0  2061638667                                             __NA__   \n",
       "1  2056600635                                             __NA__   \n",
       "2  2058253621  Free nation where citizens elect their represe...   \n",
       "3  2058997303                                             __NA__   \n",
       "4  2058184341                                             __NA__   \n",
       "5  2057930711                                             __NA__   \n",
       "6  2058524165                                             __NA__   \n",
       "7  2057837907                                             __NA__   \n",
       "8  2058736151                                             __NA__   \n",
       "9  2057900787                                             __NA__   \n",
       "\n",
       "                                                list  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "5  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "6  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "df = pd.read_csv(\"roberta_from_scratch/motn_data.csv\", encoding = 'latin-1')\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "# Creates the dataframe\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69595131-ddb0-4971-a28b-5353fb27ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define CustomDataset Class\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment_text\n",
    "        self.targets = self.data.list\n",
    "        self.CASEID = self.data.CASEID\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'caseid': self.CASEID[index],\n",
    "            'text': comment_text,\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype = torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype = torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b13c5f5-276d-4e3a-b690-6639892c834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset: (13987, 3)\n",
      "Train Dataset: (11189, 3)\n",
      "Validation Dataset: (2798, 3)\n"
     ]
    }
   ],
   "source": [
    "### Split into Train and Validation\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_val_size = 0.8\n",
    "train_dataset, val_dataset = train_test_split(new_df, test_size = 1 - train_val_size, random_state = 1337, shuffle = True)\n",
    "\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "val_dataset = val_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"Full Dataset: {}\".format(new_df.shape))\n",
    "print(\"Train Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"Validation Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "# Defining Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") # sentencepiece won't install, so not using deberta tokenizer\n",
    "\n",
    "# Create the datasets\n",
    "batch_size = 8\n",
    "train_set = CustomDataset(train_dataset, tokenizer)\n",
    "val_set = CustomDataset(val_dataset, tokenizer)\n",
    "\n",
    "params = {'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "val_loader = DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53ac7d09-4fb1-411d-9433-1484b4c87289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEBERTAClass(\n",
       "  (l1): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (l2): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create Customized Model with Dropout\n",
    "\n",
    "class DEBERTAClass(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.l1 = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.l2 = nn.Linear(self.l1.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outputs = self.l1(ids, attention_mask=mask, token_type_ids = token_type_ids)\n",
    "        last_hidden_state = outputs[0]  # Get the last hidden state\n",
    "        \n",
    "        # Pooling: Use the [CLS] token representation (first token)\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output_2 = self.dropout(pooled_output)\n",
    "        output = self.l2(output_2)\n",
    "        return output    \n",
    "    \n",
    "model = DEBERTAClass(13)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ef8081-b581-484f-98e9-053720741817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Defining the Training Function\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs = 50, batch_size = 16, learning_rate = 1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, drop_last = True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Combines sigmoid + BCE, for multi-label\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training\n",
    "        for batch in tqdm.tqdm(train_dataloader, desc = f\"Epoch {epoch + 1} - Training\"):\n",
    "            ids = batch['ids'].to(device)\n",
    "            mask = batch['mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['targets'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_exact_matches = 0\n",
    "        total_samples = 0\n",
    "        total_jaccard = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm.tqdm(val_dataloader, desc = f\"Epoch {epoch + 1} - Validation\"):\n",
    "                ids = batch['ids'].to(device)\n",
    "                mask = batch['mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                labels = batch['targets'].to(device)\n",
    "                outputs = model(ids, mask, token_type_ids)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                # Flatten tensors before appending to handle batch dimension properly\n",
    "                all_preds.append(preds.cpu().view(-1, preds.size(-1)))\n",
    "                all_labels.append(labels.cpu().view(-1, labels.size(-1)))\n",
    "                \n",
    "                # 2. Exact accuracy\n",
    "                total_exact_matches += torch.sum(torch.all(preds == labels, dim=1)).item()\n",
    "                total_samples += labels.size(0)\n",
    "                \n",
    "                # 3. Jaccard Index (intersection over union)\n",
    "                preds_bool = preds.bool()\n",
    "                labels_bool = labels.bool()\n",
    "                intersection = torch.sum(preds_bool & labels_bool, dim=1).float()\n",
    "                union = torch.sum(preds_bool | labels_bool, dim=1).float()\n",
    "                batch_jaccard = torch.mean(intersection / (union + 1e-8)).item()\n",
    "                total_jaccard += batch_jaccard\n",
    "        \n",
    "        # Calculate metrics val_loss, exact_accuracy, jaccard_accuracy\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        exact_accuracy = total_exact_matches / total_samples\n",
    "        jaccard_accuracy = total_jaccard / len(val_dataloader)\n",
    "        \n",
    "        # Calculate micro F1\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        all_preds_bool = all_preds.bool()\n",
    "        all_labels_bool = all_labels.bool()\n",
    "        \n",
    "        micro_tp = torch.sum(all_preds_bool & all_labels_bool).float()\n",
    "        micro_fp = torch.sum(all_preds_bool & ~all_labels_bool).float()\n",
    "        micro_fn = torch.sum(~all_preds_bool & all_labels_bool).float()\n",
    "        \n",
    "        micro_precision = micro_tp / (micro_tp + micro_fp + 1e-8)\n",
    "        micro_recall = micro_tp / (micro_tp + micro_fn + 1e-8)\n",
    "        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + 1e-8)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "            f\"micro_f1: {micro_f1:.4f} | exact_acc: {exact_accuracy:.4f} | jaccard: {jaccard_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"deberta_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 3: # Stop if validation loss does not improve for 3 epochs\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbc70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training: 100%|██████████| 1398/1398 [06:50<00:00,  3.41it/s]\n",
      "Epoch 1 - Validation: 100%|██████████| 349/349 [00:42<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss: 0.1489 | val_loss: 0.1124 | micro_f1: 0.7359 | exact_acc: 0.6938 | jaccard: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training:   1%|▏         | 20/1398 [00:08<09:32,  2.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m epochs = \u001b[32m50\u001b[39m\n\u001b[32m      4\u001b[39m learning_rate = \u001b[32m1e-05\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_dataset, val_dataset, num_epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m     32\u001b[39m     loss.backward()\n\u001b[32m     33\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     total_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m train_loss = total_train_loss / \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[32m     38\u001b[39m model.eval()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### Train\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "learning_rate = 1e-05\n",
    "\n",
    "trained_model = train_model(\n",
    "    model = model,\n",
    "    train_dataset = train_set,\n",
    "    val_dataset = val_set,\n",
    "    num_epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    learning_rate = learning_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
