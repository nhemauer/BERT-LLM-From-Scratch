{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796a5d-df4f-4829-bc1f-d8ef0aae6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "get_device()\n",
    "\n",
    "tokenizer_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f44a43-8a29-43fa-b364-237924d4ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define the ModelLoader, Tokenizer, Encoding, Embedding, PositionalEmbedding, MultiHeadAttention, and RobertaLikeModel Classes\n",
    "## These classes make up the original RoBERTa model architecture and tokenizer functionality from the paper\n",
    "\n",
    "# Function for loading the model and configuration\n",
    "class ModelLoader:\n",
    "    def __init__(self, config_path):\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "\n",
    "    \n",
    "# Function for loading the tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        return tokenizer\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "    \n",
    "    def decode(self, tokens, skip_special_tokens=False):\n",
    "        if isinstance(tokens, list):\n",
    "            decoded = self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n",
    "        elif isinstance(tokens, torch.Tensor):\n",
    "            decoded = self.tokenizer.decode(tokens.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "        else:\n",
    "            raise ValueError(\"Tokens must be a list or torch.Tensor\")\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokenizer.convert_tokens_to_ids(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def get_eos_token_id(self):\n",
    "        return self.tokenizer.eos_token_id\n",
    "    \n",
    "    def get_bos_token_id(self):\n",
    "        return self.tokenizer.bos_token_id\n",
    "    \n",
    "    def get_pad_token_id(self):\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        return self.tokenizer.mask_token_id\n",
    "    \n",
    "\n",
    "# Function for encoding text with special handling for beginning of sequence (BOS) token    \n",
    "class Encoding(Tokenizer):\n",
    "    def __init__(self, prompt):\n",
    "        super().__init__(tokenizer_path)\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    def enc(self):\n",
    "        encoded = self.tokenizer.encode(self.prompt)\n",
    "        tokens = encoded.ids\n",
    "        bos_token = \"<s>\"\n",
    "        bos_id = self.tokenizer.token_to_id(bos_token)\n",
    "        if bos_id is not None:\n",
    "            tokens = [bos_id] + tokens\n",
    "        else:\n",
    "            print(f\"Error: '{bos_token}' token not found in vocabulary.\")\n",
    "            print(\"Vocabulary:\", self.tokenizer.get_vocab())\n",
    "        \n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    \n",
    "# Function for creating embeddings from the model's word embeddings using config parameters\n",
    "class Embedding:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.dim = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"] \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.embedding_layer.weight.data.copy_(self.model.embeddings.word_embeddings.weight)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_layer.to(self.device)\n",
    "\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "    \n",
    "# Function for creating positional embeddings using the model's configuration    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, seq_length):\n",
    "        positions = torch.arange(seq_length, device = self.device)\n",
    "        return self.pos_embedding(positions)\n",
    "\n",
    "    \n",
    "# Function for creating multi-head attention mechanism using the model's configuration    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(config[\"hidden_size\"] / config[\"num_attention_heads\"])\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.key = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.value = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "\n",
    "    # Define a method to transpose the input tensor for multi-head attention\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.attention_head_size, dtype = torch.float))\n",
    "        \n",
    "        # Apply the attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = F.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        attention_output = self.dense(context_layer)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm(attention_output + hidden_states)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "# Putting all RoBERTa-like components together in a model class \n",
    "class RobertaLikeModel(nn.Module):\n",
    "    def __init__(self, model_loader, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = model_loader.get_config()\n",
    "        self.pretrained_model = model_loader.get_model()\n",
    "        self.encoder = self.pretrained_model.encoder # We will not be training a tokenizer on multiple terabytes of text... \n",
    "        \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.word_embedding = Embedding(self.pretrained_model, self.config)\n",
    "        self.positional_embedding = PositionalEmbedding(self.config)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.config)\n",
    "        # This can be replaced by the following line to make use of the pretrained attention weights:\n",
    "        # self.attention = self.pretrained_model.encoder.layer[0].attention\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(self.config[\"hidden_size\"], num_labels)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        encoded = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        max_len = max(len(seq) for seq in encoded)\n",
    "        padded = [seq + [self.tokenizer.get_pad_token_id()] * (max_len - len(seq)) for seq in encoded]\n",
    "        \n",
    "        input_ids = torch.tensor(padded, device=self.device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        \n",
    "        word_embeds = self.word_embedding.get_embeddings(input_ids)\n",
    "        pos_embeds = self.positional_embedding(input_ids.size(1))\n",
    "        \n",
    "        embeddings = word_embeds + pos_embeds\n",
    "        \n",
    "        attention_mask = (input_ids != self.tokenizer.get_pad_token_id()).float()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=extended_attention_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2855af-f1c5-4e4f-bcdd-af310bdaa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the CustomDataset class\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': torch.tensor(label, dtype = torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec375-e1d9-471d-a10d-c62ea0519bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset: (13987, 3)\n",
      "Train Dataset: (11190, 3)\n",
      "Test Dataset: (2797, 3)\n"
     ]
    }
   ],
   "source": [
    "### Preprocessing Data\n",
    "\n",
    "df = pd.read_csv(\"roberta_from_scratch/motn_data.csv\", encoding = 'latin-1')\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "# Remove duplicates (excluding CASEID)\n",
    "columns_to_check = ['comment_text'] + [col for col in df.columns if col not in ['CASEID', 'comment_text']]\n",
    "df = df.drop_duplicates(subset = columns_to_check)\n",
    "\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "train_size = 0.8 # 80% Train Size\n",
    "train_dataset = new_df.sample(frac = train_size, random_state = 1337)\n",
    "test_dataset = new_df.drop(train_dataset.index).reset_index(drop = True)\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"Full Dataset: {}\".format(new_df.shape))\n",
    "print(\"Train Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"Test Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset['comment_text'], train_dataset['list'], tokenizer = \"RoBERTa\")\n",
    "val_dataset = CustomDataset(test_dataset['comment_text'], test_dataset['list'], tokenizer = \"RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397646a0-3196-43ea-8bed-cc9a71bc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the Training Function\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs = 50, batch_size = 16, learning_rate = 1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, drop_last = True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Combines sigmoid + BCE, for multi-label\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training\n",
    "        for batch in tqdm(train_dataloader, desc = f\"Epoch {epoch + 1} - Training\"):\n",
    "            prompts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prompts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_exact_matches = 0\n",
    "        total_samples = 0\n",
    "        total_jaccard = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc = f\"Epoch {epoch + 1} - Validation\"):\n",
    "                prompts = batch['text']\n",
    "                labels = batch['label'].to(device)\n",
    "                outputs = model(prompts)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                # Collect predictions and labels for micro F1 calculation\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "                \n",
    "                # 1. Exact accuracy\n",
    "                total_exact_matches += torch.sum(torch.all(preds == labels, dim=1)).item()\n",
    "                total_samples += labels.size(0)\n",
    "                \n",
    "                # 2. Jaccard Index (intersection over union)\n",
    "                preds_bool = preds.bool()\n",
    "                labels_bool = labels.bool()\n",
    "                intersection = torch.sum(preds_bool & labels_bool, dim=1).float()\n",
    "                union = torch.sum(preds_bool | labels_bool, dim=1).float()\n",
    "                batch_jaccard = torch.mean(intersection / (union + 1e-8)).item()\n",
    "                total_jaccard += batch_jaccard\n",
    "        \n",
    "        # Calculate metrics val_loss, exact_accuracy, jaccard_accuracy\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        exact_accuracy = total_exact_matches / total_samples\n",
    "        jaccard_accuracy = total_jaccard / len(val_dataloader)\n",
    "        \n",
    "        # 3. Calculate micro F1\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        all_preds_bool = all_preds.bool()\n",
    "        all_labels_bool = all_labels.bool()\n",
    "        \n",
    "        micro_tp = torch.sum(all_preds_bool & all_labels_bool).float()\n",
    "        micro_fp = torch.sum(all_preds_bool & ~all_labels_bool).float()\n",
    "        micro_fn = torch.sum(~all_preds_bool & all_labels_bool).float()\n",
    "        \n",
    "        micro_precision = micro_tp / (micro_tp + micro_fp + 1e-8)\n",
    "        micro_recall = micro_tp / (micro_tp + micro_fn + 1e-8)\n",
    "        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall + 1e-8)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "            f\"micro_f1: {micro_f1:.4f} | exact_acc: {exact_accuracy:.4f} | jaccard: {jaccard_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"roberta_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 3: # Stop if validation loss does not improve for 3 epochs\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeeb5d-cdd2-46f5-a4bf-93d86a2b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train\n",
    "\n",
    "# Load model and RoBERTa Config\n",
    "config_path = \"config.json\" \n",
    "model_loader = ModelLoader(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaLikeModel(model_loader, num_labels = 13).to(get_device())\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(model, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
