# Transformer Implementations (RoBERTa, DeBERTa, LLaMA 3 7b)

This repository contains a PyTorch-based implementation of the RoBERTa transformer model and tokenizer from scratch. Additionally, I include implementations of DeBERTa, DeBERTa NLI, and LLamaV3.

With the large-scale open response survey used to train these models, LLamaV3 7b performed best.

## What's Inside

- **RoBERTa and Tokenizer Full**  
  Full implementation of the RoBERTa model, including the tokenizer and training loop, all built using PyTorch. Meant for learning how transformer architectures function at a lower level.

- **DeBERTa and DeBERTa NLI**  
  Scripts for initializing and using DeBERTa and its variant fine-tuned on natural language inference tasks.

- **LLaMA 3 7b**  
  Script for using and exploring Metaâ€™s LLaMA 3 model.

