# Transformer Implementations (RoBERTa and DeBERTa)

This repository contains a PyTorch-based implementation of the RoBERTa transformer model and tokenizer from scratch. Additionally, I include an implementation of DeBERTa.

With the large-scale open response survey used to train these models, DeBERTaV3 performed best.

## What's Inside

- **RoBERTa and Tokenizer Full**  
  Full implementation of the RoBERTa model, including the tokenizer and training loop, all built using PyTorch. Meant for learning how transformer architectures function at a lower level.

- **DeBERTa**  
  Scripts for initializing and using DeBERTa.

