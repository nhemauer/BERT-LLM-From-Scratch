{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76796a5d-df4f-4829-bc1f-d8ef0aae6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "get_device()\n",
    "\n",
    "tokenizer_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f44a43-8a29-43fa-b364-237924d4ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define the ModelLoader, Tokenizer, Encoding, Embedding, PositionalEmbedding, MultiHeadAttention, and RobertaLikeModel Classes\n",
    "## These classes make up the original RoBERTa model architecture and tokenizer functionality from the paper\n",
    "\n",
    "# Function for loading the model and configuration\n",
    "class ModelLoader:\n",
    "    def __init__(self, config_path):\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "\n",
    "    \n",
    "# Function for loading the tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        return tokenizer\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "    \n",
    "    def decode(self, tokens, skip_special_tokens=False):\n",
    "        if isinstance(tokens, list):\n",
    "            decoded = self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n",
    "        elif isinstance(tokens, torch.Tensor):\n",
    "            decoded = self.tokenizer.decode(tokens.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "        else:\n",
    "            raise ValueError(\"Tokens must be a list or torch.Tensor\")\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokenizer.convert_tokens_to_ids(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def get_eos_token_id(self):\n",
    "        return self.tokenizer.eos_token_id\n",
    "    \n",
    "    def get_bos_token_id(self):\n",
    "        return self.tokenizer.bos_token_id\n",
    "    \n",
    "    def get_pad_token_id(self):\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        return self.tokenizer.mask_token_id\n",
    "    \n",
    "\n",
    "# Function for encoding text with special handling for beginning of sequence (BOS) token    \n",
    "class Encoding(Tokenizer):\n",
    "    def __init__(self, prompt):\n",
    "        super().__init__(tokenizer_path)\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    def enc(self):\n",
    "        encoded = self.tokenizer.encode(self.prompt)\n",
    "        tokens = encoded.ids\n",
    "        bos_token = \"<s>\"\n",
    "        bos_id = self.tokenizer.token_to_id(bos_token)\n",
    "        if bos_id is not None:\n",
    "            tokens = [bos_id] + tokens\n",
    "        else:\n",
    "            print(f\"Error: '{bos_token}' token not found in vocabulary.\")\n",
    "            print(\"Vocabulary:\", self.tokenizer.get_vocab())\n",
    "        \n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    \n",
    "# Function for creating embeddings from the model's word embeddings using config parameters\n",
    "class Embedding:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.dim = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"] \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.embedding_layer.weight.data.copy_(self.model.embeddings.word_embeddings.weight)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_layer.to(self.device)\n",
    "\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "    \n",
    "# Function for creating positional embeddings using the model's configuration    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, seq_length):\n",
    "        positions = torch.arange(seq_length, device = self.device)\n",
    "        return self.pos_embedding(positions)\n",
    "\n",
    "    \n",
    "# Function for creating multi-head attention mechanism using the model's configuration    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(config[\"hidden_size\"] / config[\"num_attention_heads\"])\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.key = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.value = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "\n",
    "    # Define a method to transpose the input tensor for multi-head attention\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.attention_head_size, dtype = torch.float))\n",
    "        \n",
    "        # Apply the attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = F.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        attention_output = self.dense(context_layer)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm(attention_output + hidden_states)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "# Putting all RoBERTa-like components together in a model class \n",
    "class RobertaLikeModel(nn.Module):\n",
    "    def __init__(self, model_loader, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = model_loader.get_config()\n",
    "        self.pretrained_model = model_loader.get_model()\n",
    "        self.encoder = self.pretrained_model.encoder # We will not be training a tokenizer on multiple terabytes of text... \n",
    "        \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.word_embedding = Embedding(self.pretrained_model, self.config)\n",
    "        self.positional_embedding = PositionalEmbedding(self.config)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.config)\n",
    "        # This can be replaced by the following line to make use of the pretrained attention weights:\n",
    "        # self.attention = self.pretrained_model.encoder.layer[0].attention\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(self.config[\"hidden_size\"], num_labels)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        encoded = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        max_len = max(len(seq) for seq in encoded)\n",
    "        padded = [seq + [self.tokenizer.get_pad_token_id()] * (max_len - len(seq)) for seq in encoded]\n",
    "        \n",
    "        input_ids = torch.tensor(padded, device=self.device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        \n",
    "        word_embeds = self.word_embedding.get_embeddings(input_ids)\n",
    "        pos_embeds = self.positional_embedding(input_ids.size(1))\n",
    "        \n",
    "        embeddings = word_embeds + pos_embeds\n",
    "        \n",
    "        attention_mask = (input_ids != self.tokenizer.get_pad_token_id()).float()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=extended_attention_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2855af-f1c5-4e4f-bcdd-af310bdaa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the CustomDataset class\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Add input validation\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)  # Convert to string\n",
    "\n",
    "        # We don't need to tokenize here because the model does it internally\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': torch.tensor(label, dtype = torch.long).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec375-e1d9-471d-a10d-c62ea0519bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the Training Data\n",
    "\n",
    "df = pd.read_csv(\"motn.csv\", encoding = 'latin-1')\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "# Creates the dataframe\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()  # List are the labels\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)\n",
    "\n",
    "train_size = 0.8 # 80% Train Size\n",
    "train_dataset=new_df.sample(frac = train_size, random_state = 200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop = True)\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset.comment_text, train_dataset.list, Tokenizer._load_tokenizer(\"RoBERTa\"))\n",
    "val_dataset = CustomDataset(test_dataset.comment_text, test_dataset.list, Tokenizer._load_tokenizer(\"RoBERTa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397646a0-3196-43ea-8bed-cc9a71bc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the Training Function\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs = 10, batch_size = 8, learning_rate = 1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, drop_last = True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "            prompts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prompts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "                prompts = batch['text']\n",
    "                labels = batch['label'].to(device)\n",
    "                outputs = model(prompts)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                correct_predictions += torch.sum(preds == labels).item()\n",
    "                total_predictions += labels.numel()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        # Results\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} | train_loss: {avg_train_loss:.4f} | val_loss: {avg_val_loss:.4f} | val_accuracy: {accuracy:.4f}\"\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeeb5d-cdd2-46f5-a4bf-93d86a2b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train\n",
    "\n",
    "# Load model and RoBERTa Config\n",
    "config_path = \"config.json\" \n",
    "model_loader = ModelLoader(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaLikeModel(model_loader, num_labels = 13).to(get_device())\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(model, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
