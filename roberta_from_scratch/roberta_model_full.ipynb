{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796a5d-df4f-4829-bc1f-d8ef0aae6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "get_device()\n",
    "\n",
    "tokenizer_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f44a43-8a29-43fa-b364-237924d4ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Define the ModelLoader, Tokenizer, Encoding, Embedding, PositionalEmbedding, MultiHeadAttention, and RobertaLikeModel Classes\n",
    "## These classes make up the original RoBERTa model architecture and tokenizer functionality from the paper\n",
    "\n",
    "# Function for loading the model and configuration\n",
    "class ModelLoader:\n",
    "    def __init__(self, config_path):\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "\n",
    "    \n",
    "# Function for loading the tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        return tokenizer\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "    \n",
    "    def decode(self, tokens, skip_special_tokens=False):\n",
    "        if isinstance(tokens, list):\n",
    "            decoded = self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)\n",
    "        elif isinstance(tokens, torch.Tensor):\n",
    "            decoded = self.tokenizer.decode(tokens.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "        else:\n",
    "            raise ValueError(\"Tokens must be a list or torch.Tensor\")\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokenizer.convert_tokens_to_ids(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def get_eos_token_id(self):\n",
    "        return self.tokenizer.eos_token_id\n",
    "    \n",
    "    def get_bos_token_id(self):\n",
    "        return self.tokenizer.bos_token_id\n",
    "    \n",
    "    def get_pad_token_id(self):\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        return self.tokenizer.mask_token_id\n",
    "    \n",
    "\n",
    "# Function for encoding text with special handling for beginning of sequence (BOS) token    \n",
    "class Encoding(Tokenizer):\n",
    "    def __init__(self, prompt):\n",
    "        super().__init__(tokenizer_path)\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    def enc(self):\n",
    "        encoded = self.tokenizer.encode(self.prompt)\n",
    "        tokens = encoded.ids\n",
    "        bos_token = \"<s>\"\n",
    "        bos_id = self.tokenizer.token_to_id(bos_token)\n",
    "        if bos_id is not None:\n",
    "            tokens = [bos_id] + tokens\n",
    "        else:\n",
    "            print(f\"Error: '{bos_token}' token not found in vocabulary.\")\n",
    "            print(\"Vocabulary:\", self.tokenizer.get_vocab())\n",
    "        \n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    \n",
    "# Function for creating embeddings from the model's word embeddings using config parameters\n",
    "class Embedding:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.dim = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"] \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.embedding_layer.weight.data.copy_(self.model.embeddings.word_embeddings.weight)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_layer.to(self.device)\n",
    "\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "    \n",
    "# Function for creating positional embeddings using the model's configuration    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, seq_length):\n",
    "        positions = torch.arange(seq_length, device = self.device)\n",
    "        return self.pos_embedding(positions)\n",
    "\n",
    "    \n",
    "# Function for creating multi-head attention mechanism using the model's configuration    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(config[\"hidden_size\"] / config[\"num_attention_heads\"])\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.key = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.value = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "\n",
    "    # Define a method to transpose the input tensor for multi-head attention\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.attention_head_size, dtype = torch.float))\n",
    "        \n",
    "        # Apply the attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = F.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        attention_output = self.dense(context_layer)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm(attention_output + hidden_states)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "# Putting all RoBERTa-like components together in a model class \n",
    "class RobertaLikeModel(nn.Module):\n",
    "    def __init__(self, model_loader, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = model_loader.get_config()\n",
    "        self.pretrained_model = model_loader.get_model()\n",
    "        self.encoder = self.pretrained_model.encoder # We will not be training a tokenizer on multiple terabytes of text... \n",
    "        \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.word_embedding = Embedding(self.pretrained_model, self.config)\n",
    "        self.positional_embedding = PositionalEmbedding(self.config)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.config)\n",
    "        # This can be replaced by the following line to make use of the pretrained attention weights:\n",
    "        # self.attention = self.pretrained_model.encoder.layer[0].attention\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(self.config[\"hidden_size\"], num_labels)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        encoded = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        max_len = max(len(seq) for seq in encoded)\n",
    "        padded = [seq + [self.tokenizer.get_pad_token_id()] * (max_len - len(seq)) for seq in encoded]\n",
    "        \n",
    "        input_ids = torch.tensor(padded, device=self.device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        \n",
    "        word_embeds = self.word_embedding.get_embeddings(input_ids)\n",
    "        pos_embeds = self.positional_embedding(input_ids.size(1))\n",
    "        \n",
    "        embeddings = word_embeds + pos_embeds\n",
    "        \n",
    "        attention_mask = (input_ids != self.tokenizer.get_pad_token_id()).float()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=extended_attention_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2855af-f1c5-4e4f-bcdd-af310bdaa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the CustomDataset class\n",
    "\n",
    "class CustomDataset():\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': torch.tensor(label, dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bec375-e1d9-471d-a10d-c62ea0519bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset: (13987, 3)\n",
      "Train Dataset: (11190, 3)\n",
      "Test Dataset: (2797, 3)\n"
     ]
    }
   ],
   "source": [
    "### Preprocess the Training Data\n",
    "\n",
    "df = pd.read_csv(\"motn_data.csv\", encoding = 'latin-1')\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "# Creates the dataframe\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()  # List are the labels\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)\n",
    "\n",
    "train_size = 0.8 # 80% Train Size\n",
    "train_dataset = new_df.sample(frac = train_size, random_state = 1337)\n",
    "test_dataset = new_df.drop(train_dataset.index).reset_index(drop = True)\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"Full Dataset: {}\".format(new_df.shape))\n",
    "print(\"Train Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"Test Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset['comment_text'], train_dataset['list'], tokenizer = \"RoBERTa\")\n",
    "val_dataset = CustomDataset(test_dataset['comment_text'], test_dataset['list'], tokenizer = \"RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397646a0-3196-43ea-8bed-cc9a71bc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the Training Function\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs = 20, batch_size = 16, learning_rate = 1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, drop_last = True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Combines sigmoid + BCE, for multi-label\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Training\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} - Training\"):\n",
    "            prompts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prompts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_hamming_correct = 0\n",
    "        total_hamming_total = 0\n",
    "        total_exact_matches = 0\n",
    "        total_samples = 0\n",
    "        total_jaccard = 0\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch + 1} - Validation\"):\n",
    "                prompts = batch['text']\n",
    "                labels = batch['label'].to(device)\n",
    "                outputs = model(prompts)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                ## Metrics for multi-label classification\n",
    "                # 1. Hamming Accuracy (per-label accuracy)\n",
    "                total_hamming_correct += torch.sum(preds == labels).item()\n",
    "                total_hamming_total += labels.numel()\n",
    "                \n",
    "                # 2. Subset Accuracy (exact match)\n",
    "                total_exact_matches += torch.sum(torch.all(preds == labels, dim=1)).item()\n",
    "                total_samples += labels.size(0)\n",
    "                \n",
    "                # 3. Jaccard Index (intersection over union)\n",
    "                preds_bool = preds.bool()\n",
    "                labels_bool = labels.bool()\n",
    "                intersection = torch.sum(preds_bool & labels_bool, dim=1).float()\n",
    "                union = torch.sum(preds_bool | labels_bool, dim=1).float()\n",
    "                batch_jaccard = torch.mean(intersection / (union + 1e-8)).item()\n",
    "                total_jaccard += batch_jaccard\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        hamming_accuracy = total_hamming_correct / total_hamming_total\n",
    "        exact_accuracy = total_exact_matches / total_samples\n",
    "        jaccard_accuracy = total_jaccard / len(val_dataloader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} | train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | \"\n",
    "            f\"hamming_acc: {hamming_accuracy:.4f} | exact_acc: {exact_accuracy:.4f} | jaccard: {jaccard_accuracy:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"roberta_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 3: # Stop if validation loss does not improve for 3 epochs\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdeeb5d-cdd2-46f5-a4bf-93d86a2b02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 - Training: 100%|██████████| 699/699 [00:26<00:00, 26.64it/s]\n",
      "Epoch 1 - Validation: 100%|██████████| 174/174 [00:01<00:00, 106.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train_loss: 0.1717 | val_loss: 0.1283 | hamming_acc: 0.9503 | exact_acc: 0.5704 | jaccard: 0.5898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|██████████| 699/699 [00:28<00:00, 24.19it/s]\n",
      "Epoch 2 - Validation: 100%|██████████| 174/174 [00:01<00:00, 105.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train_loss: 0.1267 | val_loss: 0.1164 | hamming_acc: 0.9546 | exact_acc: 0.6379 | jaccard: 0.6595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|██████████| 699/699 [00:27<00:00, 25.54it/s]\n",
      "Epoch 3 - Validation: 100%|██████████| 174/174 [00:01<00:00, 93.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train_loss: 0.1172 | val_loss: 0.1091 | hamming_acc: 0.9587 | exact_acc: 0.5499 | jaccard: 0.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|██████████| 699/699 [00:26<00:00, 26.06it/s]\n",
      "Epoch 4 - Validation: 100%|██████████| 174/174 [00:01<00:00, 95.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train_loss: 0.1117 | val_loss: 0.1028 | hamming_acc: 0.9594 | exact_acc: 0.5589 | jaccard: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|██████████| 699/699 [00:30<00:00, 23.18it/s]\n",
      "Epoch 5 - Validation: 100%|██████████| 174/174 [00:02<00:00, 85.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | train_loss: 0.1072 | val_loss: 0.0980 | hamming_acc: 0.9608 | exact_acc: 0.6369 | jaccard: 0.6557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training: 100%|██████████| 699/699 [00:29<00:00, 24.02it/s]\n",
      "Epoch 6 - Validation: 100%|██████████| 174/174 [00:01<00:00, 103.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | train_loss: 0.1046 | val_loss: 0.0981 | hamming_acc: 0.9615 | exact_acc: 0.6940 | jaccard: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training:  48%|████▊     | 333/699 [00:14<00:15, 23.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model = RobertaLikeModel(model_loader, num_labels = \u001b[32m13\u001b[39m).to(get_device())\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_dataset, val_dataset, num_epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m     28\u001b[39m outputs = model(prompts)\n\u001b[32m     29\u001b[39m loss = loss_fn(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m optimizer.step()\n\u001b[32m     32\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\_tensor.py:653\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    644\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    645\u001b[39m         Tensor.backward,\n\u001b[32m    646\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    651\u001b[39m         inputs=inputs,\n\u001b[32m    652\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### Train\n",
    "\n",
    "# Load model and RoBERTa Config\n",
    "config_path = \"config.json\" \n",
    "model_loader = ModelLoader(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaLikeModel(model_loader, num_labels = 13).to(get_device())\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(model, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
