{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d810a478-5d50-4344-9ab5-1f37870cebc0",
   "metadata": {},
   "source": [
    "# MOTN Transformer July 23rd 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a56d2-0cab-456a-a292-325f044f76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from transformers import DebertaV2Model\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda6e51-feef-4790-ac6e-82ee53cb008d",
   "metadata": {},
   "source": [
    "### 1. Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e89a8e-0c9f-4383-a89d-3a45c424807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59960cf-41fb-4da6-a489-711c1044da54",
   "metadata": {},
   "source": [
    "### 2. Import Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92aeba-5b59-4e2a-bd0f-c6e31350807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/storage/home/ndh5286/Projects/MOTN Transformer/2021_2024.csv\", encoding='latin-1')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Creates the dataframe\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9043346-4bbc-4249-a4f8-c0b2138b4220",
   "metadata": {},
   "source": [
    "### 3. Defining Key Variables and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c853982-6a87-4343-8392-181d890bf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 8 #8\n",
    "VALID_BATCH_SIZE = 4 #4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "# Defining Tokenizer\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94076e-0c0a-4d69-9aa5-b61d80378045",
   "metadata": {},
   "source": [
    "### 4. Creating Dataset Object for Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69595131-ddb0-4971-a28b-5353fb27ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment_text\n",
    "        self.targets = self.data.list\n",
    "        self.CASEID = self.data.CASEID\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'caseid': self.CASEID[index],\n",
    "            'text': comment_text,\n",
    "            'ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype = torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype = torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f50ea-3119-484f-bb1a-f2b713c23431",
   "metadata": {},
   "source": [
    "### 5. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13c5f5-276d-4e3a-b690-6639892c834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_val_size = 0.8\n",
    "train_val_dataset, test_dataset = train_test_split(new_df, test_size = 1-train_val_size, random_state = 200)\n",
    "\n",
    "# Now split the remaining data into train and validation\n",
    "train_size = 0.75  # This will be 75% of 80% = 60% of total\n",
    "train_dataset, val_dataset = train_test_split(train_val_dataset, test_size = 1-train_size, random_state = 200)\n",
    "\n",
    "# Reset indices\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "val_dataset = val_dataset.reset_index(drop = True)\n",
    "test_dataset = test_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "# Create the datasets\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "validation_set = CustomDataset(val_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b9d158-d60a-4d6f-89c8-473c18d6223e",
   "metadata": {},
   "source": [
    "### 6. Setting Params and Creating Dataloader Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af4425-5697-4995-90a6-98f35ee0ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_loader = DataLoader(training_set, **params)\n",
    "val_loader = DataLoader(validation_set, **params)\n",
    "test_loader = DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c16f6f-2f89-4e9f-8ec2-42f0ebac3067",
   "metadata": {},
   "source": [
    "### 7. Defining Model with Extra Dropout and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac7d09-4fb1-411d-9433-1484b4c87289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the customized model by adding dropout\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class DEBERTAClass(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.l1 = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.l2 = nn.Linear(self.l1.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outputs = self.l1(ids, attention_mask=mask, token_type_ids = token_type_ids)\n",
    "        last_hidden_state = outputs[0]  # Get the last hidden state\n",
    "        \n",
    "        # Pooling: Use the [CLS] token representation (first token)\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output_2 = self.dropout(pooled_output)\n",
    "        output = self.l2(output_2)\n",
    "        return output    \n",
    "    \n",
    "model = DEBERTAClass(10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996bc32-6314-4894-a030-b3971f6aee41",
   "metadata": {},
   "source": [
    "### 8. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de82c9f0-a61f-45ae-bc03-6cd3475e585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698bbf6-d0bf-49f7-92fb-8a4336e7e1d9",
   "metadata": {},
   "source": [
    "### 9. Define Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef8081-b581-484f-98e9-053720741817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(train_loader, desc = f\"Epoch {epoch + 1}\"):\n",
    "        ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = (output.argmax(dim=1) == targets.argmax(dim = 1)).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss.item() / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_val_accuracy = 0\n",
    "    epoch_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device, dtype = torch.float)\n",
    "            \n",
    "            val_output = model(ids, mask, token_type_ids)\n",
    "            val_loss = loss_fn(val_output, targets)\n",
    "            \n",
    "            acc = (val_output.argmax(dim=1) == targets.argmax(dim=1)).float().mean()\n",
    "            epoch_val_accuracy += acc / len(val_loader)\n",
    "            epoch_val_loss += val_loss.item() / len(val_loader)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch : {epoch + 1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67591-ac0e-43af-b4a2-e12d131bdc26",
   "metadata": {},
   "source": [
    "### 10. Accuracy Check on Test Dataset (Holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee91c83-2179-47a0-9df4-b7a019407e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Function\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    texts = []\n",
    "    caseid = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(test_loader, desc=f\"Validation Epoch\"):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            texts.extend(batch['text'])  # Extracting the text column\n",
    "            caseid.extend(batch['caseid'])\n",
    "    \n",
    "    return fin_outputs, fin_targets, texts, caseid\n",
    "\n",
    "# For Validation (Hold out) Data\n",
    "for epoch in range(1):\n",
    "    outputs, targets, texts, caseid = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.65                                      #This can be tuned\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average = 'micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average = 'macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    #print(\"First three items of texts:\", texts[:3])\n",
    "    #print(\"First three items of fin_outputs:\", outputs[:3])\n",
    "    #print(\"First three items of fin_targets:\", targets[:3])\n",
    "    \n",
    "    # Create a dictionary with column names as keys and lists as values\n",
    "    final_df = {\n",
    "        'Outputs': outputs,\n",
    "        'Targets': targets,\n",
    "        'Texts': texts,\n",
    "        'Caseid': caseid\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24a0b9-8875-41e1-884c-41ce04e85fd6",
   "metadata": {},
   "source": [
    "### 11. Creating Output Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716703af-b96c-49bd-b540-5a1387098f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final output\n",
    "final_case = final_df['Caseid']\n",
    "final_case = pd.DataFrame(final_case, columns = [\"Caseid\"])\n",
    "\n",
    "final_text = final_df['Texts']\n",
    "final_text = pd.DataFrame(final_text, columns = [\"Text\"])\n",
    "\n",
    "final_output = final_df['Outputs']\n",
    "final_output = pd.DataFrame(final_output, columns = [\"Freedom and Rights\", \"Not a Democracy a Republic\", \"Flawed Democracy\", \"Institution and Constitution\", \"Don't Know\", \"Nothing/Disaffected\", \n",
    "                                                     \"Nothing More to Add\", \"NA\", \"Unclassified\", \"Representation and Popular Will\"])\n",
    "final_output.replace({True: 1, False: 0}, inplace=True)\n",
    "\n",
    "final_df = final_text.join(final_output)\n",
    "final_df = final_case.join(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb6ac3-8a6c-4282-82aa-9d0d95cca77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db53c7d-5803-4304-91a3-42091d1d8d8d",
   "metadata": {},
   "source": [
    "### 12. Check Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e995014-7073-48c7-a7c2-d3efd5187100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "true_np = targets\n",
    "pred_np = outputs\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = multilabel_confusion_matrix(true_np, pred_np)\n",
    "\n",
    "# Define label titles\n",
    "label_titles = [\"Freedom and Rights\", \"Not a Democracy a Republic\", \"Flawed Democracy\", \"Institution and Constitution\", \"Don't Know\", \"Nothing/Disaffected\", \n",
    "                \"Nothing More to Add\", \"NA\", \"Unclassified\", \"Representation and Popular Will\"]\n",
    "\n",
    "# Create a dictionary to store confusion matrices with titles\n",
    "conf_matrix_dict = {}\n",
    "for i, title in enumerate(label_titles):\n",
    "    conf_matrix_dict[title] = conf_matrix[i]\n",
    "\n",
    "# Print confusion matrices with titles\n",
    "for title, matrix in conf_matrix_dict.items():\n",
    "    print(f\"Confusion matrix for {title}:\")\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e406fee1-4ec9-4a04-a01c-95a76267495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"/storage/home/ndh5286/Projects/MOTN Transformer/DeBERTaV3_model_7.23.24.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb64da-df10-423e-b13d-e4eec07e2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load(\"/storage/home/ndh5286/Projects/MOTN Transformer/final_model_6.2.24.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
